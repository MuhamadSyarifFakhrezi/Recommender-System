{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhcI9_4-kc0d"
      },
      "source": [
        "***\n",
        "# **Recommender System**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqcz72GBkc0m"
      },
      "source": [
        "## Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px64_QYGkc0r"
      },
      "source": [
        "The aim of this project is to develop a recommendation system that can help customers find fashion products they like based on their previous shopping behaviour. The recommendation system will use implicit feedback data, such as purchase history and product ratings, to suggest products that may be of interest to customers.\n",
        "\n",
        "The recommendation model is built using the Alternating Least Squares (ALS) algorithm by utilising implicit feedback data. Exploratoty Data Analysis process was also conducted to find out the distribution, description, and insight of the data, Laplace Smoothing approach was used to help prevent bias towards products with few high-ranking reviews in the top products ranking process by adding a constant to each category to avoid zero probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bth1tiu_kc0v"
      },
      "source": [
        "## Data Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eadjZb1kc0x"
      },
      "source": [
        "This project utilises the [Amazon](https://amazon-reviews-2023.github.io/) dataset for fashion product categories which includes information on purchase history and product ratings by customers, dataset contains more than 2.5M product reviews in the fashion category from 2002 to 2023 (Downloadable [here](https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Amazon_Fashion.jsonl.gz)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS2-N5jrkc0y"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHGXxqX5kc00"
      },
      "source": [
        "| Field | Type | Explanation |\n",
        "| --- | --- | --- |\n",
        "| rating | float | Rating of the product (from 1.0 to 5.0). |\n",
        "| title | str | Title of the customer review. |\n",
        "| text | str | Text body of the customer review. |\n",
        "| images | list | Images that customers post after they have received the product. Each image has different sizes (small, medium, large), represented by the small_image_url, medium_image_url, and large_image_url respectively. |\n",
        "| asin | str | ID of the product. |\n",
        "| parent_asin | str | Parent ID of the product. Note: Products with different colors, styles, sizes usually belong to the same parent ID. The “asin” in previous Amazon datasets is actually parent ID. |\n",
        "| user_id | str | ID of the reviewer. |\n",
        "| timestamp | int | Time of the review (unix time). |\n",
        "| verified_purchase | bool | Customer purchase verification. |\n",
        "| helpful_vote | int | Helpful votes of the review. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAW8KDUNkc03"
      },
      "source": [
        "### Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1rNqNlCkc05"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi2DzF-Nkc08"
      },
      "source": [
        "### Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVcJtXkvkc09"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "raw_df = load_jsonl('/content/drive/MyDrive/Data Scientist/Amazon_Fashion.jsonl')\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKAAq0Wmkc1A"
      },
      "source": [
        "Since the timestamp is in code, we need to convert it to make it easier to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-v8XD1lkc1B"
      },
      "outputs": [],
      "source": [
        "raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'], unit='ms')\n",
        "raw_df['timestamp'].sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEULGZ7Qkc1C"
      },
      "source": [
        "It can be seen that the dataset contains sales from 2002 to 2023, we will only use the last 3 years data, 2020-08-01 to 2023-09-11 (the last date in the data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcejYETakc1C"
      },
      "outputs": [],
      "source": [
        "df = raw_df[(raw_df['timestamp'] >= '2020-08-01') & (raw_df['timestamp'] <= '2023-09-11')].reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFGm0_Ghkc1D"
      },
      "source": [
        "Next, select the data that will be used in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsWE9Rv1kc1F"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['title', 'text', 'images', 'parent_asin'], axis=1)\n",
        "df = df.rename(columns={'asin': 'product_id'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la-f1_Brkc1L"
      },
      "source": [
        "We check for missing value and duplicated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6nG2sLOkc1L"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhBbbQeIkc1N"
      },
      "source": [
        "There are no missing value in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elQrAH5gkc1N"
      },
      "outputs": [],
      "source": [
        "print('Duplicate Data:', df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf45-Vq3kc1O"
      },
      "source": [
        "There are 6568 duplicate data, we will delete this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaQ3IcVbkc1O"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "print('Duplicated Data:', df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikg6ye6tkc1P"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqSRMAA5kc1Q"
      },
      "source": [
        "Each column has an appropriate data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvPTsYtLkc1R"
      },
      "outputs": [],
      "source": [
        "df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvqROgEYkc1R"
      },
      "source": [
        "It appears that there are no anomalies in each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXUCHU0rkc1T"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlBkL4Jrkc1T"
      },
      "source": [
        "Let's look at the distribution of rating, verified_purchase, and helpful_vote features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH7n4xwUkc1T"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3,figsize=(15,5))\n",
        "for i, feature in enumerate(['rating', 'verified_purchase']):\n",
        "    sns.countplot(data=df, x=feature, ax=ax[i])\n",
        "    ax[i].set_title(f'Distribution of {feature}')\n",
        "    ax[i].set_xlabel(None)\n",
        "\n",
        "sns.boxplot(data=df, y='helpful_vote', ax=ax[2])\n",
        "ax[2].set_title('Distribution of helpful_vote')\n",
        "ax[2].set_ylabel('count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02cpTwhykc1X"
      },
      "source": [
        "What days and times are customers most active?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiCg5flWkc1Y"
      },
      "outputs": [],
      "source": [
        "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "ax1 = sns.countplot(data=df, x='day_of_week', order=df['day_of_week'].value_counts().index)\n",
        "plt.title('Customer Active Days')\n",
        "plt.xlabel(None)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "ax2 = sns.countplot(data=df, x='hour', order=df['hour'].value_counts().index)\n",
        "plt.title('Customer Active Hours')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr4JXrv8kc1Z"
      },
      "source": [
        "Now let's see how many ratings, unique product, and unique customers are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2goxTmaukc1b"
      },
      "outputs": [],
      "source": [
        "n_ratings = len(df)\n",
        "n_customers = df['user_id'].nunique()\n",
        "n_products = df['product_id'].nunique()\n",
        "\n",
        "print(f'Number of ratings: {n_ratings}')\n",
        "print(f'Number of customers: {n_customers}')\n",
        "print(f'Number of products: {n_products}')\n",
        "print(f'Average number of ratings per customer: {round(n_ratings/n_customers, 2)}')\n",
        "print(f'Average number of ratings per product: {round(n_ratings/n_products, 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtuTcYfLkc1c"
      },
      "source": [
        "Let's see who gave the most ratings and distribution of user rating frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK0TBHBtkc1c"
      },
      "outputs": [],
      "source": [
        "ratings_per_customer = df[['user_id', 'product_id']].groupby('user_id').count().reset_index()\n",
        "ratings_per_customer.columns = ['user_id', 'n_ratings']\n",
        "ratings_per_customer.sort_values('n_ratings', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUEgPM-kc1d"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "sns.kdeplot(ratings_per_customer['n_ratings'], fill=True, legend=False)\n",
        "plt.title(\"Number of products rated per customer\")\n",
        "plt.xlabel(\"Ratings per customer\")\n",
        "plt.axvline(ratings_per_customer['n_ratings'].mean(), color=\"k\", linestyle=\"--\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoL2dhAikc1e"
      },
      "source": [
        "What are the highest and lowest rated product?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWhNWpy6kc1e"
      },
      "outputs": [],
      "source": [
        "mean_rating = df.groupby('product_id')[['rating']].mean()\n",
        "mean_rating.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXWyEllbkc1f"
      },
      "outputs": [],
      "source": [
        "display(df.loc[df['product_id'] == mean_rating['rating'].idxmax()],\n",
        "        df.loc[df['product_id'] == mean_rating['rating'].idxmin()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-93uDTSokc1g"
      },
      "source": [
        "Although the product with id '0512238944' is the product with the highest average rating, it only received 2 reviews, which is not a good measure for a top product.\n",
        "\n",
        "We apply the Laplace Smoothing method, which is a technique that adds a constant to each category to avoid zero probability. To ensure that products with a small number of reviews do not dominate.\n",
        "\n",
        "$$ \\text{Laplace Score} = \\frac{\\sum R_i + k \\cdot R_{\\text{avg}}}{n + k} $$\n",
        "\n",
        "- ∑Ri​: Total number of rating for the product.\n",
        "- n: Number of product reviews.\n",
        "- k: Constant (the number of additional reviews we add).\n",
        "- Ravg​: Global average rating of all product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL9y-3VIkc1g"
      },
      "outputs": [],
      "source": [
        "product_ratings = df.groupby('product_id').agg(\n",
        "    rating_sum=('rating', 'sum'),\n",
        "    rating_count=('rating', 'count'),\n",
        "    rating_mean=('rating', 'mean')\n",
        ").reset_index()\n",
        "product_ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NauKCdeekc1h"
      },
      "outputs": [],
      "source": [
        "def laplace_smoothing(row, global_avg, k):\n",
        "    return (row['rating_sum'] + k * global_avg) / (row['rating_count'] + k)\n",
        "\n",
        "global_avg_rating = df['rating'].mean()\n",
        "\n",
        "product_ratings['laplace_score'] = product_ratings.apply(\n",
        "    laplace_smoothing, global_avg=global_avg_rating, k=5, axis=1\n",
        ")\n",
        "\n",
        "top_products = product_ratings.sort_values('laplace_score', ascending=False)\n",
        "top_products = top_products.set_index('product_id')\n",
        "print('Highest Product:\\n', top_products.head(10), '\\n')\n",
        "print('Lowest Product:\\n', top_products.tail(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cEK3voAkc1i"
      },
      "source": [
        "Based on the laplace score result, produk with id '0512238944' (top product before Laplace Smoothing is applied) is not visible in the top product list. The product that has the highest laplace score is the product with product_id 'B0B9144W3P', let's check the product details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBBfu3XUkc1i"
      },
      "outputs": [],
      "source": [
        "display(df[df['product_id']=='B0B9144W3P'].rating.value_counts(),\n",
        "        df[df['product_id']=='B0B9144W3P'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUNsQj2fkc1j"
      },
      "source": [
        "It appears that the product has quite good reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95U9GYKSkc1j"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NlWpFBckc1j"
      },
      "source": [
        "We select the features that will be used in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO8_yTPGkc1l"
      },
      "outputs": [],
      "source": [
        "main_df = df[['user_id', 'product_id', 'rating', 'helpful_vote', 'verified_purchase']]\n",
        "main_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vBH0InUkc1o"
      },
      "source": [
        "Then we normalise the feature rating and helpful_vote to have the same scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-w7Ggcckc1q"
      },
      "outputs": [],
      "source": [
        "main_df['helpful_vote_norm'] = main_df['helpful_vote'] / main_df['helpful_vote'].max()\n",
        "main_df['rating_norm'] = main_df['rating'] / main_df['rating'].max()\n",
        "main_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI4_jpjNkc1s"
      },
      "outputs": [],
      "source": [
        "main_df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9mxT5qkc1t"
      },
      "source": [
        "We weight the three features to get the implicit score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XvRVzFBkc1u"
      },
      "outputs": [],
      "source": [
        "main_df['implicit_score'] = (\n",
        "    0.5 * main_df['rating_norm'] +\n",
        "    0.3 * main_df['helpful_vote_norm'] +\n",
        "    0.2 * main_df['verified_purchase'].astype(int))\n",
        "main_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHxt6Ma9kc1v"
      },
      "source": [
        "And then we create a matrix that will be used to train the model with user_id as the row index, product_id as the column index, and implicit score as the value of the matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf7Gx5L2kc1v"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "C = main_df['user_id'].nunique()\n",
        "P = main_df['product_id'].nunique()\n",
        "\n",
        "customer_mapper = dict(zip(np.unique(main_df['user_id']), list(range(C))))\n",
        "product_mapper = dict(zip(np.unique(main_df['product_id']), list(range(P))))\n",
        "customer_inv_mapper = dict(zip(list(range(C)), np.unique(main_df['user_id'])))\n",
        "product_inv_mapper = dict(zip(list(range(P)), np.unique(main_df['product_id'])))\n",
        "\n",
        "row_index = [customer_mapper[i] for i in main_df['user_id']]\n",
        "col_index = [product_mapper[i] for i in main_df['product_id']]\n",
        "\n",
        "user_item_matrix = csr_matrix((main_df['implicit_score'], (row_index, col_index)), shape=(C, P))\n",
        "user_item_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WEev_0Nkc1w"
      },
      "outputs": [],
      "source": [
        "print(user_item_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-T6yGoskc1w"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3M1_D5kc1x"
      },
      "source": [
        "Let's train the model with matrix we have created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N525_rswkc1x"
      },
      "outputs": [],
      "source": [
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "model = AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50, use_gpu=False)\n",
        "model.fit(user_item_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Oh7CHtkc1z"
      },
      "source": [
        "### Top N Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o16SX4zKkc1z"
      },
      "outputs": [],
      "source": [
        "def get_recommendation(user_id, N=10):\n",
        "    if user_id in customer_mapper:\n",
        "        customer_index = customer_mapper[user_id]\n",
        "        user_items = user_item_matrix[customer_index, :].tocsr()\n",
        "        recommendations = model.recommend(customer_index, user_items, N=N)\n",
        "        product_indices, scores = recommendations\n",
        "        filtered_recommendations = [(product_index, score) for product_index, score in zip(product_indices, scores) if product_index in product_inv_mapper]\n",
        "    else:\n",
        "        popular_products = top_products['laplace_score'].index[:10]\n",
        "        filtered_recommendations = [(product_mapper[product], top_products.loc[f'{product}', 'laplace_score']) for product in popular_products if product in product_mapper]\n",
        "\n",
        "    print(f\"Top {N} Recommendations for UserId {user_id}:\")\n",
        "    for recommendation in filtered_recommendations:\n",
        "        item_index, score = recommendation[0], recommendation[1]\n",
        "        product_id = product_inv_mapper.get(item_index, \"Unknown\")\n",
        "        print(f'Product ID: {product_id}, Score: {score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnY_j-B7kc10"
      },
      "source": [
        "Let's try to get product recommendations for exiting customer with user_id 'AHTTU2FL6FCNBBAESCJHOHHSSW7A' from the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG4zbctMkc10"
      },
      "outputs": [],
      "source": [
        "if 'AHTTU2FL6FCNBBAESCJHOHHSSW7A' in main_df['user_id'].values:\n",
        "  get_recommendation('AHTTU2FL6FCNBBAESCJHOHHSSW7A')\n",
        "else:\n",
        "  print('This is a new customer')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for new customer with user_id 'TZIYHRCLTADI7R5STTUCVRE2CQMU'"
      ],
      "metadata": {
        "id": "VUbyg8VDui24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'TZIYHRCLTADI7R5STTUCVRE2CQMU' not in main_df['user_id'].values:\n",
        "  get_recommendation('TZIYHRCLTADI7R5STTUCVRE2CQMU')\n",
        "else:\n",
        "  print('This is not a new customer')"
      ],
      "metadata": {
        "id": "GZamk_iOuy-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}